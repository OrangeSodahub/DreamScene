# SceneCraft: Layout-Guided 3D Scene Generation

<a href="https://github.com/OrangeSodahub" style="color:blue;">Xiuyu Yang*</a> ·
<a href="https://yunzeman.github.io/" style="color:blue;">Yunze Man*</a> ·
<a href="https://scholar.google.com/citations?user=_m5__wUAAAAJ" style="color:blue;">Jun-Kun Chen</a> ·
<a href="https://yxw.web.illinois.edu/" style="color:blue;">Yu-Xiong Wang</a>

**[NeurIPS 2024]** [[`Project Page`](https://orangesodahub.github.io/SceneCraft/)] [[`arXiv`](http://arxiv.org/abs/)] [[`pdf`](https://arxiv.org/pdf/)] [[`BibTeX`](#BibTex)] [[`License`](https://github.com/OrangeSodahub/SceneCraft?tab=MIT-1-ov-file)]


## About
<img src="assets/teaser.gif" width="100%"/>
We introduce SceneCraft, an innovative framework for generating complex, detailed indoor scenes from textual descriptions and spatial layouts. By leveraging a rendering-based pipeline, and a layout-conditioned diffusion model, our work effectively converts 3D semantic layouts into multi-view 2D images and learns a final scene representation that is not only consistent and realistic but also adheres closely to user specifications. Please check out project page and paper for more details. The open-source release of our code and dataset promises to further empower research and development in this exciting domain. <br><br>


## TODO

*Codes and models will be soon, stay tuned ! Please check out project page and paper for more details.*


## BibTeX
If you find our work useful in your research, please consider citing our paper:
```bibtex
@article{TBD,
      title={Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding},
      author={Yang, Xiuyu and Man, Yunze and Chen, Jun-Kun and Wang, Yu-Xiong},
      journal={arXiv preprint arXiv:TBD},
      year={2024} 
}
```